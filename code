import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import pandas as pd
import ast
import matplotlib.pyplot as plt

# 데이터 로드 및 전처리
df = pd.read_excel("/content/drive/MyDrive/스타일선호벡터정보표_22.xlsx")
df['Training_스타일_선호_벡터'] = df['Training_스타일_선호_벡터'].fillna('[]').apply(ast.literal_eval)
df['Training_스타일_비선호_벡터'] = df['Training_스타일_비선호_벡터'].fillna('[]').apply(ast.literal_eval)
df['Validation_스타일_선호_벡터'] = df['Validation_스타일_선호_벡터'].fillna('[]').apply(ast.literal_eval)
df['Validation_스타일_비선호_벡터'] = df['Validation_스타일_비선호_벡터'].fillna('[]').apply(ast.literal_eval)

# 타겟 벡터 길이 설정
target_length = max(
    max(df['Training_스타일_선호_벡터'].apply(len)),
    max(df['Training_스타일_비선호_벡터'].apply(len)),
    max(df['Validation_스타일_선호_벡터'].apply(len)),
    max(df['Validation_스타일_비선호_벡터'].apply(len)),
)

# 벡터 패딩/자르기 함수
def pad_or_truncate(vector, length):
    if len(vector) < length:
        return vector + [0] * (length - len(vector))
    else:
        return vector[:length]

# 패딩/자르기 적용
df['Training_스타일_선호_벡터'] = df['Training_스타일_선호_벡터'].apply(lambda x: pad_or_truncate(x, target_length))
df['Training_스타일_비선호_벡터'] = df['Training_스타일_비선호_벡터'].apply(lambda x: pad_or_truncate(x, target_length))
df['Validation_스타일_선호_벡터'] = df['Validation_스타일_선호_벡터'].apply(lambda x: pad_or_truncate(x, target_length))
df['Validation_스타일_비선호_벡터'] = df['Validation_스타일_비선호_벡터'].apply(lambda x: pad_or_truncate(x, target_length))

# NumPy 배열로 변환
training_preference_vectors = np.array(df['Training_스타일_선호_벡터'].tolist(), dtype=np.float32)
training_non_preference_vectors = np.array(df['Training_스타일_비선호_벡터'].tolist(), dtype=np.float32)

# 학습 및 검증 데이터 준비
X_train = np.concatenate((training_preference_vectors, training_non_preference_vectors), axis=0)
y_train = np.array([1] * len(training_preference_vectors) + [0] * len(training_non_preference_vectors), dtype=np.float32)

# 텐서로 변환
X_train = torch.FloatTensor(X_train)
y_train = torch.FloatTensor(y_train)

# 강화학습 환경 정의
class StylePreferenceEnv:
    def __init__(self, data, labels, target_length):
        self.data = data
        self.labels = labels
        self.target_length = target_length
        self.current_step = 0
        self.state = self.data[self.current_step]

    def reset(self):
        self.current_step = 0
        self.state = self.data[self.current_step]
        return self.state

    def step(self, action):
        reward = 1.0 if action == self.labels[self.current_step] else -1.0
        self.current_step += 1
        if self.current_step >= len(self.data):
            done = True
            self.current_step = 0  # Reset for new episode
        else:
            done = False
            self.state = self.data[self.current_step]
        
        return self.state, reward, done

# DQN 모델 정의
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 128)  # 추가된 히든 레이어
        self.fc4 = nn.Linear(128, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# 패딩 함수
def pad_sequences(sequences, max_len=None):
    if max_len is None:
        max_len = max(len(seq) for seq in sequences)
    padded_sequences = []
    for seq in sequences:
        if isinstance(seq, torch.Tensor):
            seq = seq.tolist()
        if len(seq) < max_len:
            padded_seq = seq + [0] * (max_len - len(seq))
        else:
            padded_seq = seq[:max_len]
        padded_sequences.append(padded_seq)
    return np.array(padded_sequences)

# DQN 에이전트 정의
class DQNAgent:
    def __init__(self, state_dim, action_dim, learning_rate=0.0001, gamma=0.95, epsilon=0.1, epsilon_min=0.1, epsilon_decay=0.997):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.memory = []
        self.model = self.build_model()
        self.optimizer = optim.RMSprop(self.model.parameters(), lr=self.learning_rate)  # RMSprop 적용
        self.criterion = nn.SmoothL1Loss()
        self.losses = []

    def build_model(self):
        model = DQN(self.state_dim, self.action_dim)
        return model

    def act(self, state):
        if random.uniform(0, 1) < self.epsilon:
            return random.randrange(self.action_dim)
        state = torch.FloatTensor(state).unsqueeze(0)
        q_values = self.model(state)
        return torch.argmax(q_values).item()

    def remember(self, state, action, reward, next_state, done):
        if isinstance(state, torch.Tensor):
            state = state.tolist()
        if isinstance(next_state, torch.Tensor):
            next_state = next_state.tolist()
        self.memory.append((state, action, reward, next_state, done))

    def replay(self, batch_size):
        if len(self.memory) < batch_size:
            return
        batch = random.sample(self.memory, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        # 패딩 적용
        max_len = max(len(state) for state in states)
        states = pad_sequences(states, max_len)
        next_states = pad_sequences(next_states, max_len)

        states = torch.FloatTensor(states)
        next_states = torch.FloatTensor(next_states)
        actions = torch.LongTensor(np.array(actions))
        rewards = torch.FloatTensor(np.array(rewards))
        dones = torch.FloatTensor(np.array(dones))

        q_values = self.model(states)
        next_q_values = self.model(next_states)
        target_q_values = rewards + self.gamma * torch.max(next_q_values, dim=1)[0] * (1 - dones)

        q_value = q_values[range(batch_size), actions]
        loss = self.criterion(q_value, target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.losses.append(loss.item())

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

def evaluate_agent(agent, env, num_episodes=10):
    total_rewards = []
    for _ in range(num_episodes):
        state = env.reset()
        done = False
        episode_reward = 0
        while not done:
            action = agent.act(state)
            next_state, reward, done = env.step(action)
            episode_reward += reward
            state = next_state
        total_rewards.append(episode_reward)
    return np.mean(total_rewards)

# 학습 설정
state_dim = target_length
action_dim = 2  # 선호(1) 또는 비선호(0)
agent = DQNAgent(state_dim, action_dim, learning_rate=0.0001)

# 학습 루프
num_episodes = 100
batch_size = 64
env = StylePreferenceEnv(X_train, y_train, target_length)

episode_rewards = []
eval_interval = 50

for episode in range(num_episodes):
    state = env.reset()
    done = False
    total_reward = 0
    while not done:
        action = agent.act(state)
        next_state, reward, done = env.step(action)
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward
        
        if len(agent.memory) > batch_size:
            agent.replay(batch_size)
    
    episode_rewards.append(total_reward)
    print(f"Episode {episode + 1}/{num_episodes} completed. Total reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.4f}")
    
    if (episode + 1) % eval_interval == 0:
        mean_reward = evaluate_agent(agent, env)
        print(f"Evaluation after episode {episode + 1}: Mean reward = {mean_reward:.2f}")

# 모델 저장
torch.save(agent.model.state_dict(), '/content/drive/MyDrive/dqn_style_preference_model.pth')

# 학습 결과 시각화
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(episode_rewards)
plt.title('Episode Rewards')
plt.xlabel('Episode')
plt.ylabel('Total Reward')

plt.subplot(1, 2, 2)
plt.plot(agent.losses)
plt.title('Training Loss')
plt.xlabel('Training steps')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()
